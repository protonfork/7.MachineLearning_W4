---
title: 'Machine Learning Project : Humain Activity Recognition'
author: "Nicolas"
date: "14 novembre 2016"
output:
  html_document: default
  pdf_document: default
---

```{r setup, message=FALSE,echo=FALSE,error=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(dplyr)
library(GGally)
library(corrplot)
library(caret)
library(xgboost)
library(Ckmeans.1d.dp)
set.seed(1234)
```

## Summary
The above study consist into the selection of a machine learning model for the classification of observations related to Weight Lifting Exercise. Data set provided has been splitted in a train and test sets. Train set model has been developped and cross validated using random forest algorithm and XgBoost, then selected based on the accurary on the test set. 

Although Random Forest shown good result with a >99% accuracy, XgBoost has been selected considering a slightly better accurary with a smaller calculation time. Several features has reveled themselves to be important for classification, in particular XXXXxxx, XXXxxx and XxxxxXX with gather XX% of impact on the selected model.

Finaly, provided validation set of 20 observations has been classified using the trained model.

## Processing

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.  In this project, our goal has been to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.data frm http://groupware.les.inf.puc-rio.br/har

```{r Source of Data,cache=TRUE }
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              "training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              "test.csv")
training <- read.csv("training.csv",stringsAsFactors = FALSE)
```

From the initial package of data, we first will subset the elements to keep only variables,  without NA (only non numerical value being the name of participant and response column). In addition, we removed any elements related to the timestamp, as the search model is timeset independant [considering that the expected prediction will be done on 20 time idependant observation]. 

```{r Subset of Source of Data,cache=TRUE }
library(corrplot)
sub.training <- training %>% select_if(is.numeric) %>% 
        select_if(function(col)all(!is.na(col))) %>%
        dplyr::select(-raw_timestamp_part_1,-raw_timestamp_part_2,-num_window,-X)
```

Correlation between features, as shown in the following figures, is in vast majority in good chape for later analysis, with most of the feature being square.

```{r Correlation Plot}
cor.sub.training <- cor(sub.training)
corrplot(cor.sub.training, order = "hclust", tl.col="black", tl.cex=.45)
```

We filter the feature set, limiting the correlation factor to 0.85. It will limit risks on model calculation, and slightly increase the calculation.

```{r check for highly correlated features}
HighCorr <- findCorrelation(cor.sub.training, 0.85)
sub.filtered.training <- sub.training[,-HighCorr]
sub.filtered.training <- sub.training
```

## Model Training
We trained two models for classificiation, with multiclass (5 expected) :

- A random forest model
- A XgBoost model

In order to compare both models, the filtered training set is splitted in training and test set. Test set will be used to compare the models.

In addition, both are crossvalidated using a 4 fold resample.

```{r training models random forest (ranger package), cache=TRUE }
trainIndex <- createDataPartition(training$classe,p=.8,list=F,times=1)
train.set <- sub.filtered.training[trainIndex,]
test.set <- sub.filtered.training[-trainIndex,]

train.labels <- as.factor(training$classe[trainIndex])
test.labels <- as.factor(training$classe[-trainIndex])

ctrl <- trainControl(method = "cv", number = 4, returnResamp = "all", search = "random",classProbs = T)

ranger.fit <- train(y=train.labels,x=train.set, method = "ranger", trControl = ctrl, metric = "Accuracy", importance = "permutation", verbose=F)
```

```{r training models  XG Boost, cache=TRUE}
train.xgb <- xgb.DMatrix(data.matrix(train.set), label=(as.numeric(train.labels)-1), missing=NA)

params.xgb <- list(
        "eta"=0.1, 
        "max_depth"=15,
        "subsamble"=0.8,
        "colsample_bytree"=0.8,
        "eval_metric"="merror",
        "objective"="multi:softprob",
        "num_class"=5,
        "nthread"=4,
        "min_child_weigth"=12
)

xgb.fit.cv <- xgb.cv(param=params.xgb, data=train.xgb, nfold=4, nrounds=300, prediction=TRUE, verbose=F)

best.rank <- which.min(xgb.fit.cv$dt[, test.merror.mean]) 

xgb.fit <- xgboost(param=params.xgb,data = train.xgb,nrounds=best.rank, verbose=F)

```

## model selection

Here under confugion matrix shows a sligth advantage for the XgBoost algoritm. Accuraty on the test set of both models are very good, and we expect a good result on the validation set of 20 observations.

As a consequence, XGboost models is kept as better solution for classification of this example.

```{r Confusion Matrix RF }

pred.ranger <- predict(ranger.fit,newdata=test.set)

confusionMatrix(test.labels,pred.ranger)
```

```{r Confusion Matrix XGBoost }

test.xgb <- xgb.DMatrix(data.matrix(test.set), label=(as.numeric(test.labels)-1), missing=NA)
pred.xgb <- predict(xgb.fit,newdata=test.xgb)
pred.xgb <- matrix(pred.xgb,nrow=length(pred.xgb)/5,ncol=5,byrow=T)
confusionMatrix((test.labels),c("A","B","C","D","E")[max.col(pred.xgb,"last")])
```

## Result and Validation Set Prediction

XgBoost selected model has sort the feature by importance for the classification of the observations. Here under table list the 10  feature that are more impacting for the classification.

```{r Importance of Features, cache = TRUE}
feat_imp <- xgb.importance(names(train.set),model=xgb.fit)
xgb.plot.importance(feat_imp[1:10,],1)+theme(legend.position = "none")
```

Provided set of 20 observations are then classified. Result is shown here under.

```{r Validation Set Prediction with XGBoost}
validation <- read.csv("test.csv",stringsAsFactors = FALSE)
t_validation <- validation[,names(data.frame(sub.filtered.training))]
pred.xgb <- predict(xgb.fit,newdata=data.matrix(t_validation))
pred.xgb <- matrix(pred.xgb,nrow=length(pred.xgb)/5,ncol=5,byrow=T)
validation.prediction <- c("A","B","C","D","E")[max.col(pred.xgb,"last")]
validation.prediction
```

